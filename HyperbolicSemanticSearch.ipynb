{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from d2l import torch as d2l\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainChance = 0.6\n",
    "validationChance = 0.1\n",
    "testChance = 0.3\n",
    "\n",
    "file = open(\"SICK_annotated.txt\", \"r\")\n",
    "\n",
    "trainSet = open(\"TrainDataset.txt\", \"x\")\n",
    "validationSet = open(\"ValidationDataset.txt\", \"x\")\n",
    "testSet = open(\"TestDataset.txt\", \"x\")\n",
    "\n",
    "for i, line in enumerate(file):\n",
    "    if i == 0:\n",
    "        continue\n",
    "\n",
    "    arr = line.split('\\t')\n",
    "\n",
    "    num = random.uniform(0, 1)\n",
    "\n",
    "    if num <= trainChance:\n",
    "        trainSet.write(str(arr[2] + \"\\t\" + arr[4] + \"\\t\" + arr[6] + \"\\n\"))\n",
    "    elif num <= trainChance + validationChance:\n",
    "        validationSet.write(str(arr[2] + \"\\t\" + arr[4] + \"\\t\" + arr[6] + \"\\n\"))\n",
    "    else:\n",
    "        testSet.write(str(arr[2] + \"\\t\" + arr[4] + \"\\t\" + arr[6] + \"\\n\"))\n",
    "\n",
    "    #print(f\"{arr[2]} {arr[4]} {arr[6]}\")\n",
    "\n",
    "trainSet.close()\n",
    "validationSet.close()\n",
    "testSet.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile = open(\"TrainDataset.txt\", \"r\")\n",
    "validationFile = open(\"ValidationDataset.txt\", \"r\")\n",
    "testFile = open(\"TestDataset.txt\", \"r\")\n",
    "\n",
    "trainSet = []\n",
    "validationSet = []\n",
    "testSet = []\n",
    "\n",
    "for i, line in enumerate(trainFile):\n",
    "    arr = line.split('\\t')\n",
    "    arr[2] = torch.tensor(float(arr[2]))\n",
    "    trainSet.append(arr)\n",
    "\n",
    "for i, line in enumerate(validationFile):\n",
    "    arr = line.split('\\t')\n",
    "    arr[2] = torch.tensor(float(arr[2]))\n",
    "    validationSet.append(arr)\n",
    "\n",
    "for i, line in enumerate(testFile):\n",
    "    arr = line.split('\\t')\n",
    "    arr[2] = torch.tensor(float(arr[2]))\n",
    "    testSet.append(arr)\n",
    "\n",
    "print(f\"{len(trainSet)} {len(validationSet)} {len(testSet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, arr):\n",
    "        super().__init__()\n",
    "        self.data = arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_tanh(x, clamp=15):\n",
    "    return x.clamp(-clamp, clamp).tanh()\n",
    "\n",
    "def expmap0(u, c = 1, min_norm = 1e-15):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), min_norm)\n",
    "    gamma_1 = clamp_tanh(sqrt_c * u_norm) * u / (sqrt_c * u_norm)\n",
    "    return gamma_1\n",
    "\n",
    "def sqdist(p1, p2, c = 1):\n",
    "        sqrt_c = c ** 0.5\n",
    "        dist_c = torch.atanh(\n",
    "            sqrt_c * mobius_add(-p1, p2, c, dim=-1).norm(dim=-1, p=2, keepdim=False)\n",
    "        )\n",
    "        dist = dist_c * 2 / sqrt_c\n",
    "        return dist ** 2\n",
    "\n",
    "def mobius_add(x, y, c = 1, dim=-1, min_norm = 1e-15):\n",
    "        x2 = x.pow(2).sum(dim=dim, keepdim=True)\n",
    "        y2 = y.pow(2).sum(dim=dim, keepdim=True)\n",
    "        xy = (x * y).sum(dim=dim, keepdim=True)\n",
    "        num = (1 + 2 * c * xy + c * y2) * x + (1 - c * x2) * y\n",
    "        denom = 1 + 2 * c * xy + c ** 2 * x2 * y2\n",
    "        return num / denom.clamp_min(min_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchNetwork(nn.Module):\n",
    "    def __init__(self, hyperbolic, explicit, device = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "        self.fineTune = nn.Sequential(nn.Linear(768, 768), nn.Linear(768, 768)).to(device)\n",
    "        self.explicit = explicit\n",
    "        self.hyperbolic = hyperbolic\n",
    "        self.device = device\n",
    "\n",
    "        if not explicit:\n",
    "            self.regression = nn.Sequential(nn.Linear(1536, 768), nn.Linear(768, 1)).to(device)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        y1 = self.fineTune(torch.tensor(self.embedding.encode(x1)).to(self.device))\n",
    "        y2 = self.fineTune(torch.tensor(self.embedding.encode(x2)).to(self.device))\n",
    "\n",
    "        if self.hyperbolic:\n",
    "            y1 = expmap0(y1)\n",
    "            y2 = expmap0(y2)\n",
    "\n",
    "        if not self.explicit:\n",
    "            return self.regression(torch.cat((y1, y2), dim=-1))\n",
    "\n",
    "        if self.hyperbolic:\n",
    "            return sqdist(y1, y2).to(self.device)\n",
    "\n",
    "        return (y1 - y2).pow(2).sum(-1).sqrt()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(prediction, target) -> float:\n",
    "    return (prediction - target).pow(2)\n",
    "\n",
    "def absError(prediction, target) -> float:\n",
    "    return torch.abs(prediction - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network: nn.Module, dataset, lossFunc = mse, evalFunc = absError, device = None):\n",
    "    network.eval()\n",
    "\n",
    "    #Loss, evaluation metric\n",
    "    metric = d2l.Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x1, x2, y) in enumerate(dataset):\n",
    "            y = y.to(device)\n",
    "\n",
    "            yhat = network(x1, x2).to(device)\n",
    "\n",
    "            loss = torch.mean(lossFunc(yhat, y))\n",
    "            eval = torch.mean(evalFunc(yhat, y))\n",
    "\n",
    "            metric.add(loss, eval)\n",
    "\n",
    "    return metric[0] / len(dataset), metric[1] / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network: nn.Module, trainDataset, validationDataset, testDataset, learnRate, epochs, modelFileName, epochsToSave=10, lossFunc = mse, evalFunc = absError, device = None):\n",
    "    network.train()\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learnRate)\n",
    "\n",
    "    bestLoss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "\n",
    "        #Loss, evaluation metric\n",
    "        metric = d2l.Accumulator(2)\n",
    "\n",
    "        for i, (x1, x2, y) in enumerate(trainDataset):\n",
    "            y = y.to(device)\n",
    "            yhat = network(x1, x2)\n",
    "\n",
    "            loss = torch.mean(lossFunc(yhat, y))\n",
    "            eval = torch.mean(evalFunc(yhat, y))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metric.add(torch.mean(loss), eval)\n",
    "\n",
    "        validationLoss, validationEval = evaluate(network, validationDataset, device=device)\n",
    "        testLoss, testEval = evaluate(network, testDataset, device=device)\n",
    "\n",
    "        if validationLoss < bestLoss:\n",
    "            bestLoss = validationLoss\n",
    "            torch.save(network.state_dict(), modelFileName + \"BestLoss\")\n",
    "\n",
    "        if (epoch + 1) % epochsToSave == 0:\n",
    "            torch.save(network.state_dict(), modelFileName + \"Epoch\" + str(epoch))\n",
    "\n",
    "        print(f\"Epoch: {epoch}\\n\\t Train Loss: {metric[0] / len(trainDataset)}\\t\\tTrain Eval: {metric[1] / len(trainDataset)}\\n\\t \\\n",
    "Validation Loss: {validationLoss}\\t\\tValidation Eval: {validationEval}\\n \\\n",
    "\\tTest Loss: {testLoss}\\t\\tTest Eval: {testEval}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "learnRate = 0.0000001\n",
    "epochs = 25\n",
    "modelFileName = \"EuclideanImplicit5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = SentenceDataset(trainSet)\n",
    "trainIter = DataLoader(trainDataset, shuffle=True, batch_size=batchSize)\n",
    "\n",
    "validationDataset = SentenceDataset(validationSet)\n",
    "validationIter = DataLoader(validationDataset, batch_size=batchSize)\n",
    "\n",
    "testDataset = SentenceDataset(testSet)\n",
    "testIter = DataLoader(testDataset, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "net = SearchNetwork(True, True, device=device)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, trainIter, validationIter, testIter, learnRate, epochs, modelFileName, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentPath = \"Documents/\"\n",
    "fileNames = os.listdir(documentPath)\n",
    "\n",
    "maxSentences = 250\n",
    "\n",
    "punctuation = '\\.|;|\\?|!'\n",
    "\n",
    "#1: average, 2: lowest\n",
    "mode = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: different geometric space for information extraction\n",
      "0: ReinforcementLearningHumanPreferences.pdf\n",
      "1: HyperbolicRelevanceMatching.pdf\n",
      "2: ChainOfThoughtPrompting.pdf\n",
      "3: FactualAssociations.pdf\n",
      "4: ContextAwareModeling.pdf\n",
      "5: FewShotRelationExtraction.pdf\n",
      "6: PromptTransferTextGeneration.pdf\n",
      "7: FeedForwardLayersKeyValue.pdf\n"
     ]
    }
   ],
   "source": [
    "userInput = input(\"Please input your query (type quit to exit): \")\n",
    "\n",
    "while userInput.lower() != \"quit\":\n",
    "    clear_output()\n",
    "    vals = []\n",
    "\n",
    "    for name in fileNames:\n",
    "        print(f\"Reading {name}\")\n",
    "        if mode == 1:\n",
    "            vals.append(0)\n",
    "        elif mode == 2:\n",
    "            vals.append(float('inf'))\n",
    "\n",
    "        file = open(documentPath + name, 'rb')\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        numSentences = 0\n",
    "\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            if numSentences >= maxSentences:\n",
    "                break\n",
    "\n",
    "            arr = re.split(punctuation, page.extract_text())[:maxSentences - numSentences]\n",
    "            numSentences += len(arr)\n",
    "\n",
    "            for sentence in arr:\n",
    "                dist = net(sentence, userInput)\n",
    "\n",
    "                if mode == 1:\n",
    "                    vals[-1] += dist\n",
    "                elif mode == 2:\n",
    "                    vals[-1] = min(dist, vals[-1])\n",
    "\n",
    "        if mode == 1:\n",
    "            vals[-1] /= numSentences\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    newFileNames = [x for _,x in sorted(zip(vals, fileNames))]\n",
    "\n",
    "    clear_output()\n",
    "    print(f\"Query: {userInput}\")\n",
    "    for i, name in enumerate(newFileNames):\n",
    "        print(f\"{i}: {name}\")\n",
    "\n",
    "    userInput = input(\"Please input your query (type quit to exit): \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
